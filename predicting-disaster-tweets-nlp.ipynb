{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started). \n\n![nlp-getting-started image](https://storage.googleapis.com/kaggle-media/competitions/nlp1-cover.jpg)\n","metadata":{}},{"cell_type":"markdown","source":"Our goal here to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t.","metadata":{}},{"cell_type":"markdown","source":"What's in the notebook?\n\n    Exploratory Data Analysis (EDA)\n    Text Prosessing & Data Cleaning\n    Evaluation","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# visualization packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, ImageColorGenerator\n\nimport os\nimport re\nimport tqdm.notebook as tqdm\n\n# sklearn\nimport sklearn\n\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.feature_selection import SelectKBest, chi2, SelectFromModel\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV, ShuffleSplit, RepeatedStratifiedKFold, KFold, cross_val_score\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.tree import DecisionTreeClassifier\n# xgboost\nfrom xgboost import XGBClassifier, XGBRFClassifier\n# keras\nimport tensorflow as tf\n# nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n# transformers\nfrom transformers import AutoTokenizer,TFBertModel\n# packages need to be downloaded\n!pip install pyspellchecker;\nfrom spellchecker import SpellChecker\n\n# settings\nplt.style.use('ggplot')\nget_ipython().run_line_magic('matplotlib', 'inline') #ensure in-line plotting\nstop = set(stopwords.words('english'))\nsns.color_palette(\"rocket_r\", as_cmap=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-12T07:10:01.183839Z","iopub.execute_input":"2022-02-12T07:10:01.184109Z","iopub.status.idle":"2022-02-12T07:10:09.044886Z","shell.execute_reply.started":"2022-02-12T07:10:01.184078Z","shell.execute_reply":"2022-02-12T07:10:09.043986Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/nlp-getting-started/train.csv', index_col=0)\ndf_test = pd.read_csv('../input/nlp-getting-started/test.csv', index_col=0)\nsubmission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\", index_col=0)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:09.048954Z","iopub.execute_input":"2022-02-12T07:10:09.049156Z","iopub.status.idle":"2022-02-12T07:10:09.104877Z","shell.execute_reply.started":"2022-02-12T07:10:09.049130Z","shell.execute_reply":"2022-02-12T07:10:09.104019Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"## Data analysis\n\nIn the following we're going to see some data analysis on the corpus.\n\nSpecifically:\n\n    General dataset infos\n        Number of samples\n        Data Columns\n        Class Label Distributiom\n\n    Text analysis\n        Top words in positive & negative tweets\n        Hashtag Analysis\n        The word cloud","metadata":{}},{"cell_type":"code","source":"print(f\"Number of train samples {df.shape[0]}\")\nprint(f\"Number of test samples {df_test.shape[0]}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N = np.random.randint(0, df.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:09.106523Z","iopub.execute_input":"2022-02-12T07:10:09.106839Z","iopub.status.idle":"2022-02-12T07:10:09.113019Z","shell.execute_reply.started":"2022-02-12T07:10:09.106769Z","shell.execute_reply":"2022-02-12T07:10:09.111874Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum(axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:09.115222Z","iopub.execute_input":"2022-02-12T07:10:09.115876Z","iopub.status.idle":"2022-02-12T07:10:09.135763Z","shell.execute_reply.started":"2022-02-12T07:10:09.115833Z","shell.execute_reply":"2022-02-12T07:10:09.135001Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":"### Class Label Distributiom","metadata":{}},{"cell_type":"code","source":"n = df.target.value_counts();\nplt.figure(figsize=(10, 8));\nsns.barplot(n.index, n, palette=\"mako\").set_ylabel('samples');","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:09.138859Z","iopub.execute_input":"2022-02-12T07:10:09.139533Z","iopub.status.idle":"2022-02-12T07:10:09.408803Z","shell.execute_reply.started":"2022-02-12T07:10:09.139488Z","shell.execute_reply":"2022-02-12T07:10:09.408089Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"def get_top_tweet_bigrams(df, column_name='text', target=1, n=None):\n    txt = df[df['target']==target][column_name].str.lower()\n    vec = CountVectorizer(ngram_range=(1, 1)).fit(txt)\n    bag_of_words = vec.transform(txt)\n    sum_words = bag_of_words.sum(axis=0) \n    \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef visuaize_top10(top, title=''):\n    plt.figure(figsize=(12,10))\n    x, y = map( list, zip(*top) )\n    n = np.arange(len(x))\n    \n    plt.figure(figsize=(15,10))\n    #plt.barh(x, y, align='center', alpha=0.2)\n    sns.barplot(x=y, y=x, palette=\"mako\");\n    plt.plot(y, n, '-o', markersize=5, alpha=0.8)\n    plt.yticks(n, x);\n    plt.xlabel('$\\chi^2$');\n    plt.title(title)\n    \nvisuaize_top10(get_top_tweet_bigrams(df, 'text', 0, n=10), 'Top 10 words in negative tweets')\nvisuaize_top10(get_top_tweet_bigrams(df, 'text', 1, n=10), 'Top 10 words in positive tweets')","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:09.410118Z","iopub.execute_input":"2022-02-12T07:10:09.410937Z","iopub.status.idle":"2022-02-12T07:10:11.023709Z","shell.execute_reply.started":"2022-02-12T07:10:09.410893Z","shell.execute_reply":"2022-02-12T07:10:11.023039Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":"### Hashtag Analysis","metadata":{}},{"cell_type":"code","source":"def find_hashtags(tweet):\n    return \", \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or None\n\ndef add_hashtags(df):\n    df['hashtag'] = df[\"text\"].apply(lambda x: find_hashtags(x))\n    df['hashtag'].fillna(value=\"no\", inplace=True)\n    return df\n    \n\nhashtags_df = add_hashtags(df)\n_l = len([v for v in df.hashtag.values if isinstance(v, str)])\nprint(f\"-Number of tweets with hashtags: {_l}\")\n\nvisuaize_top10(\n    get_top_tweet_bigrams(hashtags_df, 'hashtag', 0, n=50),\n    'Top 10 hashtags in positive tweets'\n)\n\nvisuaize_top10(\n    get_top_tweet_bigrams(hashtags_df, 'hashtag', 1, n=50),\n    'Top 10 hashtags in positive tweets'\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:11.025262Z","iopub.execute_input":"2022-02-12T07:10:11.025717Z","iopub.status.idle":"2022-02-12T07:10:13.039140Z","shell.execute_reply.started":"2022-02-12T07:10:11.025679Z","shell.execute_reply":"2022-02-12T07:10:13.038317Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"There is too much intersection between hashtag in positive and negative samples, meaning that an\n#hashtag approach will not work that well.","metadata":{}},{"cell_type":"code","source":"def word_cloud(df, target):\n    targets = {\n        0: 'negative',\n        1: 'positive',\n    }\n    #create instance of the WordCloud() \n    word_cloud=WordCloud(height=1080, width=2048, background_color='white')\n\n    #get the text in a big string\n    txt = df.text[df['target']==target].str.lower()\n    text=\" \".join([str(word) for word in txt])\n\n    #generate the word cloud\n    word_cloud.generate(text)\n\n    #display now\n    plt.figure(figsize=(14, 12));\n    plt.imshow(word_cloud);\n    plt.axis(\"off\");\n    plt.title(f\"Most {targets[target]} Common Words\");\n    \nword_cloud(df, target=0)\nword_cloud(df, target=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:13.040755Z","iopub.execute_input":"2022-02-12T07:10:13.041012Z","iopub.status.idle":"2022-02-12T07:10:23.520759Z","shell.execute_reply.started":"2022-02-12T07:10:13.040975Z","shell.execute_reply":"2022-02-12T07:10:23.520085Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"## Text Processing and Data Cleaning\nHere we clean our dataset. Specifically, we clean:\n\n    URLs\n    HTMLs\n    punctuations\n    whitespaces\n    stopwords\n    \nand apply following ideas:\n\n    convert text to lower\n    replace emoji and smileys by their corresponding text\n    Chatwords conversion\n    correct spellings\n    lemmatizing\n    stemming","metadata":{}},{"cell_type":"markdown","source":"### Remove URL","metadata":{}},{"cell_type":"code","source":"def remove_url(text):\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\nremove_url(df.text[120])","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:23.522274Z","iopub.execute_input":"2022-02-12T07:10:23.522701Z","iopub.status.idle":"2022-02-12T07:10:23.529860Z","shell.execute_reply.started":"2022-02-12T07:10:23.522666Z","shell.execute_reply":"2022-02-12T07:10:23.529124Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"markdown","source":"### Remove HTML","metadata":{}},{"cell_type":"code","source":"example = \"\"\"<div>\n<h1>head</h1>\n<p>paragraph</p>\n<a href=\"https://www.kaggle.com/\">url</a>\n</div>\"\"\"\n\ndef remove_html(text):\n    html_pattern = re.compile(r'<.*?>')\n    return html_pattern.sub(r'', text)\n\nremove_html(example)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:23.534275Z","iopub.execute_input":"2022-02-12T07:10:23.534759Z","iopub.status.idle":"2022-02-12T07:10:23.541999Z","shell.execute_reply.started":"2022-02-12T07:10:23.534712Z","shell.execute_reply":"2022-02-12T07:10:23.541273Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"### Replace emoji and smileys by their corresponding text\nHere we scrap emoticons and their corresponding text using BeautifulSoup from [emoticonr](https://www.emoticonr.com/emoticons)","metadata":{}},{"cell_type":"code","source":"from urllib.request import urlopen\nfrom bs4 import BeautifulSoup\n\nurl = \"https://www.emoticonr.com/emoticons\"  # change to whatever your url is\n\npage = urlopen(url).read()\nsoup = BeautifulSoup(page)\n\nsmileys = {}\nfor div in soup.find_all(\"div\", {\"class\": \"tableText\"}):\n    divs = div.find_all(\"div\")\n    smileys[divs[1].text] = divs[0].text\n    \n#\nsmileys_df = pd.DataFrame(\n    smileys.items(), \n    columns = ['smiley', 'meaning'],\n    index=np.arange(len(smileys))\n)\nsmileys_df","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:23.543570Z","iopub.execute_input":"2022-02-12T07:10:23.544100Z","iopub.status.idle":"2022-02-12T07:10:25.223819Z","shell.execute_reply.started":"2022-02-12T07:10:23.544061Z","shell.execute_reply":"2022-02-12T07:10:25.223075Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"import emoji\n\ndef replace_emoji(text):\n    #replace emojis by their corresponding text\n    text = emoji.demojize(text, delimiters=(\"\", \"\"))\n    #replace smileys by their corresponding text\n    new_text = \"\"\n    for w in text.split():\n        if w.upper() in smileys:\n            new_text = new_text + smileys_df.meaning[smileys_df.smiley==w.upper()].values[0] + \" \"\n        else:\n            new_text = new_text + w + \" \"\n    return new_text\n\ntext = \"game is on 🔥 :-) 😀 :|  \"\n\nprint(text) # with emoji\nprint(replace_emoji(text))","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:25.225156Z","iopub.execute_input":"2022-02-12T07:10:25.225653Z","iopub.status.idle":"2022-02-12T07:10:25.235656Z","shell.execute_reply.started":"2022-02-12T07:10:25.225597Z","shell.execute_reply":"2022-02-12T07:10:25.233656Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"### Remove punctuation","metadata":{}},{"cell_type":"code","source":"def remove_punc(text):\n    # remove '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n    return re.sub(r'[^\\w\\s]', '', text)\n     \n\ntext = \"Gfg, is best : for ! Geeks ;\"\nremove_punc(text)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:25.237121Z","iopub.execute_input":"2022-02-12T07:10:25.237764Z","iopub.status.idle":"2022-02-12T07:10:25.246443Z","shell.execute_reply.started":"2022-02-12T07:10:25.237726Z","shell.execute_reply":"2022-02-12T07:10:25.245546Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"### Remove whitespace from text","metadata":{}},{"cell_type":"code","source":"# remove whitespace from text\ndef remove_whitespace(text):\n    return  \" \".join(text.split())\n\nremove_whitespace(\"   abc abc ab c  \")","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:25.248046Z","iopub.execute_input":"2022-02-12T07:10:25.248422Z","iopub.status.idle":"2022-02-12T07:10:25.257000Z","shell.execute_reply.started":"2022-02-12T07:10:25.248386Z","shell.execute_reply":"2022-02-12T07:10:25.256120Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"markdown","source":"### Remove Non-ASCI","metadata":{}},{"cell_type":"code","source":"def remove_non_ascii(text):\n    \"\"\"\n        Remove non-ASCII characters \n    \"\"\"\n    return re.sub(r'[^\\x00-\\x7f]',r'', text)\n    #return ''.join([x for x in text if x in string.printable]) \nremove_non_ascii('Ǹ text Ǹ')","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:25.258571Z","iopub.execute_input":"2022-02-12T07:10:25.258970Z","iopub.status.idle":"2022-02-12T07:10:25.268508Z","shell.execute_reply.started":"2022-02-12T07:10:25.258931Z","shell.execute_reply":"2022-02-12T07:10:25.267708Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"### Chatwords conversion\nHere we scrap chatwords and their meanings using BeautifulSoup from [englishclub](https://www.englishclub.com/esl-chat/abbreviations.htm)","metadata":{}},{"cell_type":"code","source":"from urllib.request import urlopen\nfrom bs4 import BeautifulSoup\n\nurl = \"https://www.englishclub.com/esl-chat/abbreviations.htm\"  # change to whatever your url is\n\npage = urlopen(url).read()\nsoup = BeautifulSoup(page)\nlen(soup.find_all('table'))\n\nchatwords = {}\nsoup.find_all('table')[1]\nfor table in soup.find_all('table'):\n    for tr in table.find_all('tr'):\n        tds = tr.find_all('td')\n        chatwords[tds[0].text.upper()] = tds[1].text\n        \nchatwords_df = pd.DataFrame(\n    chatwords.items(), \n    columns = ['abbreviation', 'meaning'],\n    index=np.arange(len(chatwords))\n)\nchatwords_df","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:25.270022Z","iopub.execute_input":"2022-02-12T07:10:25.270310Z","iopub.status.idle":"2022-02-12T07:10:25.867719Z","shell.execute_reply.started":"2022-02-12T07:10:25.270274Z","shell.execute_reply":"2022-02-12T07:10:25.867023Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"# Chatwords conversion\ndef convert_chat_words(text):\n    new_text = \"\"\n    for w in text.split():\n        if w.upper() in chatwords:\n            new_text = new_text + chatwords_df.meaning[chatwords_df.abbreviation==w.upper()].values[0] + \" \"\n        else:\n            new_text = new_text + w + \" \"\n    return new_text\n\nconvert_chat_words(\"one minute BRB\")","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:25.868879Z","iopub.execute_input":"2022-02-12T07:10:25.870183Z","iopub.status.idle":"2022-02-12T07:10:25.879461Z","shell.execute_reply.started":"2022-02-12T07:10:25.870144Z","shell.execute_reply":"2022-02-12T07:10:25.878746Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"markdown","source":"### Correct Spellings","metadata":{}},{"cell_type":"code","source":"def correct_spellings(text):\n    spell = SpellChecker()\n    # find words that may be misspelled\n    misspelled_words = spell.unknown(text.split())\n    # a list of `likely` options\n    corrected_text = []\n    \n    for word in text.split():\n        if word in misspelled_words:\n            # Get the one `most likely` answer\n            correct = spell.correction(word)\n            corrected_text.append(correct)\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntext = \"corect me plese\"\ncorrect_spellings(text)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:25.880514Z","iopub.execute_input":"2022-02-12T07:10:25.880787Z","iopub.status.idle":"2022-02-12T07:10:26.010248Z","shell.execute_reply.started":"2022-02-12T07:10:25.880748Z","shell.execute_reply":"2022-02-12T07:10:26.009386Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":"### word tokenizer\n- remove non alpha \n- remove stopwords (such as “the”, “a”, “an”, “in”)","metadata":{}},{"cell_type":"code","source":"def word_tokenizer(text):\n    \"\"\"\n    word tokenize\n    remove non alpha \n    remove stopwords (such as “the”, “a”, “an”, “in”)\n    \"\"\"\n    words = [word for word in word_tokenize(text) if((word not in stop)&(word.isalpha())&(len(word)>1))]\n    #words = [word for word in word_tokenize(text) if((word.isalpha())&(len(word)>1))]\n    return ' '.join(words)\n\nword_tokenizer(df.text.iloc[N])","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:26.011929Z","iopub.execute_input":"2022-02-12T07:10:26.012220Z","iopub.status.idle":"2022-02-12T07:10:26.020403Z","shell.execute_reply.started":"2022-02-12T07:10:26.012164Z","shell.execute_reply":"2022-02-12T07:10:26.019392Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"markdown","source":"# Lemmatisation and stemming","metadata":{}},{"cell_type":"markdown","source":"Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.\n\n    For instance:\n\n        The word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n\n        The word \"walk\" is the base form for word \"walking\", and hence this is matched in both stemming and lemmatisation.\n\n        The word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context, e.g., \"in our last meeting\" or \"We are meeting again tomorrow\". Unlike stemming, lemmatisation can in principle select the appropriate lemma depending on the context.\n\n","metadata":{}},{"cell_type":"code","source":"def do_stemming(text):\n    #Create instance of a PorterStemmer\n    #stemmer = PorterStemmer()\n    stemmer = SnowballStemmer(language='english')\n    new_str = \"\"\n    for word in text.split():\n        tokenizer = word_tokenizer(word)\n        new_str = new_str + stemmer.stem(tokenizer) + \" \"\n    return new_str\n\nprint(df.text.iloc[N])\nprint(do_stemming(df.text.iloc[N]))","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:26.021973Z","iopub.execute_input":"2022-02-12T07:10:26.022250Z","iopub.status.idle":"2022-02-12T07:10:26.034518Z","shell.execute_reply.started":"2022-02-12T07:10:26.022213Z","shell.execute_reply":"2022-02-12T07:10:26.033562Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"def do_lemmatizing(text):\n    #Create instance of a PorterStemmer\n    lemmatizer = WordNetLemmatizer()\n    new_str = \"\"\n    for word in text.split():\n        tokenizer = word_tokenizer(word)\n        new_str = new_str + lemmatizer.lemmatize(tokenizer) + \" \"\n    return new_str\n\nprint(df.text.iloc[N])\nprint(do_lemmatizing(df.text.iloc[N]))","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:26.036262Z","iopub.execute_input":"2022-02-12T07:10:26.036580Z","iopub.status.idle":"2022-02-12T07:10:26.048504Z","shell.execute_reply.started":"2022-02-12T07:10:26.036541Z","shell.execute_reply":"2022-02-12T07:10:26.047763Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"def clean_text(df, df_test, normalize='lemmatize'):\n    X = pd.concat([df, df_test])\n    \n    X.text = X.text.str.lower()\n    X.text = X.text.apply(lambda text: remove_url(text))\n    X.text = X.text.apply(lambda text: remove_html(text))\n    X.text = X.text.apply(lambda text: remove_non_ascii(text))\n    X.text = X.text.apply(lambda text: replace_emoji(text))\n    X.text = X.text.apply(lambda text: remove_punc(text))\n    X.text = X.text.apply(lambda text: convert_chat_words(text))\n    X.text = X.text.apply(lambda text: remove_whitespace(text))\n    #X.text = X.text.apply(lambda text: correct_spellings(text))  \n    if normalize == 'lemmatize':\n        X.text = X.text.apply(lambda text: do_lemmatizing(text))\n    if normalize == 'stem':\n        X.text = X.text.apply(lambda text: do_stemming(text))\n    \n    X_test = X.loc[df_test.index, :]\n    X.drop(X_test.index, inplace=True)\n    return X, X_test\n    \nnew_df, new_df_test = clean_text(df, df_test)\ndf.text.iloc[0], new_df.text.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:26.049554Z","iopub.execute_input":"2022-02-12T07:10:26.050625Z","iopub.status.idle":"2022-02-12T07:10:39.953988Z","shell.execute_reply.started":"2022-02-12T07:10:26.050597Z","shell.execute_reply":"2022-02-12T07:10:39.953084Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"new_df.text.isnull().sum(), new_df_test.text.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:39.955405Z","iopub.execute_input":"2022-02-12T07:10:39.955670Z","iopub.status.idle":"2022-02-12T07:10:39.963011Z","shell.execute_reply.started":"2022-02-12T07:10:39.955639Z","shell.execute_reply":"2022-02-12T07:10:39.962297Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"visuaize_top10(get_top_tweet_bigrams(new_df, 'text', 0, n=50), 'Top 10 words in negative tweets After text processing and data cleaning')\nvisuaize_top10(get_top_tweet_bigrams(new_df, 'text', 1, n=50), 'Top 10 words in positive tweets After text processing and data cleaning')","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:39.964632Z","iopub.execute_input":"2022-02-12T07:10:39.965185Z","iopub.status.idle":"2022-02-12T07:10:42.256324Z","shell.execute_reply.started":"2022-02-12T07:10:39.965146Z","shell.execute_reply":"2022-02-12T07:10:42.255645Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"word_cloud(new_df, target=0)\nword_cloud(new_df, target=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:42.260177Z","iopub.execute_input":"2022-02-12T07:10:42.264342Z","iopub.status.idle":"2022-02-12T07:10:52.234721Z","shell.execute_reply.started":"2022-02-12T07:10:42.264299Z","shell.execute_reply":"2022-02-12T07:10:52.234090Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"markdown","source":"### Chi2 Feature Selection\n    \nIn Scikit-learn library, there are three methods you can use for feature selection with sparse matrices such as Tfidf vectors or count vectors. By looking at the documentation, you can see that chi2, mutual_info_regression, mutual_info_classif will deal with the data without making it dense. In my case, I have 1.5 million tweets and want to reduce dimensions from 100,000 features, thus transform this into dense matrices is not an option. It will not fit into my RAM.\n\n    The chi-squared statistic measures the lack of independence between a feature (in this case, one term within a tweet) and class (whether the tweets are positive or negative).","metadata":{}},{"cell_type":"code","source":"tvec = TfidfVectorizer(max_features=100000,ngram_range=(1, 1))\nx_tfidf = tvec.fit_transform(new_df.text)\nchi2score = chi2(x_tfidf, new_df.target)[0]\ntopchi2 = zip(tvec.get_feature_names(), chi2score)\nchi2_df = pd.DataFrame(topchi2, columns=['feature', 'chi2']).sort_values(by='chi2', ascending=False)[:20]\nchi2_df.reset_index(inplace=True, drop=True)\nchi2_df","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:52.236313Z","iopub.execute_input":"2022-02-12T07:10:52.236753Z","iopub.status.idle":"2022-02-12T07:10:52.413997Z","shell.execute_reply.started":"2022-02-12T07:10:52.236711Z","shell.execute_reply":"2022-02-12T07:10:52.413266Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.barh(chi2_df.feature, chi2_df.chi2, align='center', alpha=0.2)\nplt.plot(chi2_df.chi2, chi2_df.index, '-o', markersize=5, alpha=0.8)\nplt.yticks(chi2_df.index, chi2_df.feature);\nplt.xlabel('$\\chi^2$');","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:52.415317Z","iopub.execute_input":"2022-02-12T07:10:52.415716Z","iopub.status.idle":"2022-02-12T07:10:52.774725Z","shell.execute_reply.started":"2022-02-12T07:10:52.415677Z","shell.execute_reply":"2022-02-12T07:10:52.774050Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"markdown","source":"![](https://miro.medium.com/max/900/1*3-KkZ0hlRZxjMn7Z6uXDGg.png)","metadata":{}},{"cell_type":"markdown","source":"### Evaluation:\n\n    tfidf & Classic ml classifiers like SVC, LogisticRegression...\n    Simple LSTM Model\n    Glove Bi-LSTM\n    BERT\n    \n TF-IDF:\n \n     - a numerical statistic that reflect how important a word is to a document in a collection or corpus.\n     - TfidfVectorizer equivalent to CountVectorizer followed by TfidfTransformer.","metadata":{}},{"cell_type":"markdown","source":"    Select KBest — This is used to select the top k features from the sorted feature importance’s sorted in decreasing order.\n    \n    SelectFromModel — This is used to select the feature importance’s from a model so that they can be used to train another model.","metadata":{}},{"cell_type":"code","source":"X = new_df.text\nX_test = new_df_test.text\ny = np.array(new_df.target).reshape((-1,1)).ravel()\n\n#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:52.779216Z","iopub.execute_input":"2022-02-12T07:10:52.779511Z","iopub.status.idle":"2022-02-12T07:10:52.786375Z","shell.execute_reply.started":"2022-02-12T07:10:52.779478Z","shell.execute_reply":"2022-02-12T07:10:52.785595Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"# define the evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n#cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n#cv = KFold(n_splits=5)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:52.787424Z","iopub.execute_input":"2022-02-12T07:10:52.788031Z","iopub.status.idle":"2022-02-12T07:10:52.796750Z","shell.execute_reply.started":"2022-02-12T07:10:52.787989Z","shell.execute_reply":"2022-02-12T07:10:52.796023Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"def gridsearch_model(model, parameters, X, y, X_test):\n    grid = GridSearchCV(model, parameters, cv=cv, refit=True, n_jobs=-1, verbose=1)\n    grid.fit(X, y.ravel())\n    # print best parameter after tuning\n    print(grid.best_params_)\n    # print how our model looks after hyper-parameter tuning\n    print(grid.best_estimator_)\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y, X_test=X_test):\n    # evaluate the model and collect the results\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    print(f\"Score {type(model.named_steps['clf']).__name__} is {scores.mean():.2f}\")\n    #print(f\"Score is {scores.mean():.2f}\")\n\ndef my_pipeline(clf):\n    return Pipeline([\n        #('tfid', TfidfVectorizer()),\n        ('vect', CountVectorizer()),\n        ('tfidf', TfidfTransformer()),\n        ('skb', SelectKBest(chi2, k = 13000)),\n        #('lsvc', SelectFromModel(LogisticRegression())),\n        ('clf', clf),\n    ])\n\ndef Save_Submit(model): \n    predict = model.fit(X, y).predict(X_test)\n    submission.target = predict.astype(int)\n    submission.to_csv(f\"{type(model.named_steps['clf']).__name__}_submit.csv\")\n    print(\"Saved!\")","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:52.798147Z","iopub.execute_input":"2022-02-12T07:10:52.798675Z","iopub.status.idle":"2022-02-12T07:10:52.809422Z","shell.execute_reply.started":"2022-02-12T07:10:52.798637Z","shell.execute_reply":"2022-02-12T07:10:52.808640Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"############################# SVC #######################################\nsvc_clf = my_pipeline(clf=SVC())\nsvc_param_grid = {\n    'clf__kernel': ['linear', 'poly', 'rbf', 'sigmoid'], \n    'clf__C': [0.1, 1, 10, 100, 1000],\n    'clf__gamma':  [1, 0.1, 0.01, 0.001, 0.0001],\n}\n#gridsearch_model(svc_clf, svc_param_grid, X, y, X_test)\n######################## LogisticRegression #############################\nlog_clf = my_pipeline(clf=LogisticRegression())\nlog_param_grid = {\n    'clf__C': [0.001, 0.01, 0.05, 0.1, 0,5, 1]\n}\n#gridsearch_model(log_clf, log_param_grid, X, y, X_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:52.810606Z","iopub.execute_input":"2022-02-12T07:10:52.810808Z","iopub.status.idle":"2022-02-12T07:10:52.823412Z","shell.execute_reply.started":"2022-02-12T07:10:52.810782Z","shell.execute_reply":"2022-02-12T07:10:52.822709Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"############################# SVC #######################################\nsvc_best_params = {'C': 1, 'gamma': 1, 'kernel': 'rbf'}\nsvc_clf = my_pipeline(SVC(**svc_best_params))\nevaluate_model(svc_clf, X, y)\n#Save_Submit(svc_clf)\n######################## LogisticRegression #############################\nlog_best_params = {'C': 1}\nlog_clf = my_pipeline(LogisticRegression(**log_best_params))\nevaluate_model(log_clf, X, y)\n#Save_Submit(log_clf)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:10:52.824943Z","iopub.execute_input":"2022-02-12T07:10:52.825470Z","iopub.status.idle":"2022-02-12T07:12:49.391278Z","shell.execute_reply.started":"2022-02-12T07:10:52.825426Z","shell.execute_reply":"2022-02-12T07:12:49.390291Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":"### Voting Classifier\nThe idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.","metadata":{}},{"cell_type":"code","source":"voting_clf = Pipeline([\n    ('clf', VotingClassifier(\n            estimators=[\n                ('scv', svc_clf), \n                ('log', log_clf), \n            ],\n            #voting='hard'\n        )\n    )\n])\nevaluate_model(voting_clf, X, y)\nSave_Submit(voting_clf)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:12:49.393457Z","iopub.execute_input":"2022-02-12T07:12:49.393757Z","iopub.status.idle":"2022-02-12T07:14:48.636328Z","shell.execute_reply.started":"2022-02-12T07:12:49.393710Z","shell.execute_reply":"2022-02-12T07:14:48.635485Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"markdown","source":"![](https://miro.medium.com/max/900/1*3-KkZ0hlRZxjMn7Z6uXDGg.png)","metadata":{}},{"cell_type":"markdown","source":"Basically, we will convert the tweets in the dataset to their index form by using the texts_to_sequences function available with tokenizer. After that, we will pad the sequences so all of them have the same length.","metadata":{}},{"cell_type":"code","source":"#MAX_LEN: the maximum length of one tweet we will use for our training.\n#MAX_WORDS: value to pad all the inputs to have the same length\n\nMAX_LEN = max([len(x.split()) for x in new_df.text]) \nMAX_WORDS = 750 \nembeddings_init = np.zeros((MAX_WORDS,MAX_LEN))","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:14:48.637755Z","iopub.execute_input":"2022-02-12T07:14:48.638002Z","iopub.status.idle":"2022-02-12T07:14:48.651272Z","shell.execute_reply.started":"2022-02-12T07:14:48.637966Z","shell.execute_reply":"2022-02-12T07:14:48.650580Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = MAX_WORDS)\ntokenizer.fit_on_texts(X)\n\nX_pad = tf.keras.preprocessing.sequence.pad_sequences(\n    tokenizer.texts_to_sequences(X), \n    maxlen = MAX_LEN\n) # (7613, 100)\n\nX_test_pad = tf.keras.preprocessing.sequence.pad_sequences(\n    tokenizer.texts_to_sequences(X_test),\n    maxlen = MAX_LEN\n) # (3263, 100)\n\nX_pad_train, X_pad_val, y_train, y_val = train_test_split(X_pad, y, test_size=0.2)\nX_test_pad.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:14:48.652536Z","iopub.execute_input":"2022-02-12T07:14:48.652963Z","iopub.status.idle":"2022-02-12T07:14:48.973009Z","shell.execute_reply.started":"2022-02-12T07:14:48.652924Z","shell.execute_reply":"2022-02-12T07:14:48.972360Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":"   **word_index** is a dictionary mapping words to their respective index\n","metadata":{}},{"cell_type":"code","source":"word_index= tokenizer.word_index","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:14:48.974312Z","iopub.execute_input":"2022-02-12T07:14:48.974550Z","iopub.status.idle":"2022-02-12T07:14:48.979339Z","shell.execute_reply.started":"2022-02-12T07:14:48.974517Z","shell.execute_reply":"2022-02-12T07:14:48.978364Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"markdown","source":"# NN","metadata":{}},{"cell_type":"code","source":"def train_model(X, y, model, batch_size=32, epochs=100, learning_rate=6e-06):\n    # Compile the model\n    model.compile(\n        optimizer = tf.keras.optimizers.Adam(\n            learning_rate = learning_rate, # this learning rate is for bert model.\n            epsilon = 1e-08,\n            decay = 0.01,\n            clipnorm = 1.0\n        ),\n        loss = tf.keras.losses.BinaryCrossentropy(),\n        metrics = tf.keras.metrics.BinaryAccuracy('accuracy'),\n    )\n    early_stopping = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        min_delta=0,\n        patience=15,\n        verbose=0, \n        mode='auto'\n    )\n    history = history = model.fit(\n        X, \n        y, \n        epochs = epochs, \n        batch_size = batch_size, \n        validation_split = 0.2,\n        callbacks = [early_stopping]\n    )\n    return history","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:14:48.980979Z","iopub.execute_input":"2022-02-12T07:14:48.981592Z","iopub.status.idle":"2022-02-12T07:14:48.990687Z","shell.execute_reply.started":"2022-02-12T07:14:48.981442Z","shell.execute_reply":"2022-02-12T07:14:48.989934Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"def visual_validation_and_accuracy(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs_plot = np.arange(1, len(loss) + 1)\n    plt.clf()\n    plt.figure(figsize=(10, 8))\n    plt.plot(epochs_plot, acc, 'r', label='Training acc')\n    plt.plot(epochs_plot, val_acc, 'b', label='Validation acc')\n    plt.plot(epochs_plot, loss, 'r:', label='Training loss')\n    plt.plot(epochs_plot, val_loss, 'b:', label='Validation loss')\n    plt.title('Validation and accuracy')\n    plt.xlabel('Epochs')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:14:48.991797Z","iopub.execute_input":"2022-02-12T07:14:48.992632Z","iopub.status.idle":"2022-02-12T07:14:49.004072Z","shell.execute_reply.started":"2022-02-12T07:14:48.992593Z","shell.execute_reply":"2022-02-12T07:14:49.003296Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"markdown","source":"## Dense Model","metadata":{}},{"cell_type":"code","source":"dense_model = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(MAX_WORDS, 100, input_length=MAX_LEN),\n    tf.keras.layers.Conv1D(filters=32, kernel_size=8, activation='relu'),\n    tf.keras.layers.MaxPooling1D(pool_size=2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(10, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n#dense_model.summary()\n#history = train_model(X_pad, y, dense_model)\n#visual_validation_and_accuracy(history)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:14:49.005290Z","iopub.execute_input":"2022-02-12T07:14:49.005873Z","iopub.status.idle":"2022-02-12T07:14:49.059881Z","shell.execute_reply.started":"2022-02-12T07:14:49.005837Z","shell.execute_reply":"2022-02-12T07:14:49.059178Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"markdown","source":"## LSTM Model\n\n     LSTM(Long Term Short Memory), it is a type of RNN architecture(Recurrent Neural Network) which are extensively used nowadays for NLP because it handles long sequence dependencies well.","metadata":{}},{"cell_type":"code","source":"lstm_model = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(MAX_WORDS, 32, input_length=MAX_LEN),\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    tf.keras.layers.SpatialDropout1D(0.2),\n    tf.keras.layers.LSTM(32),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n#lstm_model.summary()\n#history = train_model(X_pad, y,lstm_model)\n#visual_validation_and_accuracy(history)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:14:49.062008Z","iopub.execute_input":"2022-02-12T07:14:49.062390Z","iopub.status.idle":"2022-02-12T07:14:49.482117Z","shell.execute_reply.started":"2022-02-12T07:14:49.062355Z","shell.execute_reply":"2022-02-12T07:14:49.481383Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"markdown","source":"### GloVe Word Embeddings\n\n    GloVe is an unsupervised learning algorithm to learn vector representation i.e word embedding for various words. GloVe stands for Global Vectors for Word Representations. ","metadata":{}},{"cell_type":"markdown","source":"Now we will write a function to read the contents of the GloVe Vector file, which returns us a dictionary that maps the words to their respective word embeddings.\n\n","metadata":{}},{"cell_type":"code","source":"def read_glove_vector(glove_vec):\n    with open(glove_vec, 'r', encoding='UTF-8') as f:\n        words = set()\n        word_to_vec_map = {}\n        for line in f:\n            w_line = line.split()\n            curr_word = w_line[0]\n            word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n    return word_to_vec_map\n\nword_to_vec_map = read_glove_vector('../input/glove6b50dtxt/glove.6B.50d.txt')\nembed_vector_len = word_to_vec_map['moon'].shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:14:49.483205Z","iopub.execute_input":"2022-02-12T07:14:49.483436Z","iopub.status.idle":"2022-02-12T07:14:55.221729Z","shell.execute_reply.started":"2022-02-12T07:14:49.483402Z","shell.execute_reply":"2022-02-12T07:14:55.220947Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":"We defined the embedding matrix, where all the words which are not in the GloVe dictionary being assigned a zero vector.","metadata":{}},{"cell_type":"code","source":"vocab_len = len(word_index)\n\nembedding_matrix  = np.zeros((vocab_len+1, embed_vector_len))\n\nfor word, index in word_index.items():\n    embedding_vector = word_to_vec_map.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index, :] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:14:55.223306Z","iopub.execute_input":"2022-02-12T07:14:55.223617Z","iopub.status.idle":"2022-02-12T07:14:55.256419Z","shell.execute_reply.started":"2022-02-12T07:14:55.223575Z","shell.execute_reply":"2022-02-12T07:14:55.255654Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"glove_model = tf.keras.models.Sequential([\n    tf.keras.layers.Input(name='inputs', shape=[MAX_LEN]),\n    tf.keras.layers.Embedding(\n        input_dim = vocab_len+1, \n        output_dim = embed_vector_len, \n        input_length = MAX_LEN, \n        weights = [embedding_matrix], \n        trainable = False\n    ),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(32, activation = 'relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation = 'sigmoid'),\n])\n#glove_model.summary()\n#history = train_model(X_pad, y,glove_model, learning_rate=0.01)\n#visual_validation_and_accuracy(history)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:14:55.257675Z","iopub.execute_input":"2022-02-12T07:14:55.258024Z","iopub.status.idle":"2022-02-12T07:14:55.703522Z","shell.execute_reply.started":"2022-02-12T07:14:55.257984Z","shell.execute_reply":"2022-02-12T07:14:55.702774Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"markdown","source":"### Keras BERT using TFHub Trial \n\n    BERT can take as input either one or two sentences, and uses the special token [SEP] to differentiate them. The [CLS] token always appears at the start of the text, and is specific to classification tasks.\n    \n![bert](https://miro.medium.com/max/1400/1*OZj7CKwdlIZR8ywkx8nn2w.png)    ","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')\nbert = TFBertModel.from_pretrained('bert-large-uncased')","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:14:55.704808Z","iopub.execute_input":"2022-02-12T07:14:55.705073Z","iopub.status.idle":"2022-02-12T07:14:59.500584Z","shell.execute_reply.started":"2022-02-12T07:14:55.705025Z","shell.execute_reply":"2022-02-12T07:14:59.499862Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"print(tokenizer('Happy learning and keep kaggling &*&*&&'))","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:14:59.502240Z","iopub.execute_input":"2022-02-12T07:14:59.502780Z","iopub.status.idle":"2022-02-12T07:14:59.508710Z","shell.execute_reply.started":"2022-02-12T07:14:59.502708Z","shell.execute_reply":"2022-02-12T07:14:59.507945Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"markdown","source":"Encode the text into tokens, masks, and segment flags.","metadata":{}},{"cell_type":"code","source":"MAX_LEN = max([len(x.split()) for x in df.text]) \n\nbert_text = tokenizer(\n    text = df.text.tolist(),\n    add_special_tokens = True,\n    max_length = MAX_LEN,\n    truncation = True,\n    padding = True, \n    return_tensors = 'tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True\n)\nbert_text_test = tokenizer(\n    text = df_test.text.tolist(),\n    add_special_tokens = True,\n    max_length = MAX_LEN,\n    truncation = True,\n    padding = True, \n    return_tensors = 'tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True\n)\nbert_target = df.target.values\nbert_text['input_ids'].shape, bert_text['attention_mask'].shape","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:14:59.510278Z","iopub.execute_input":"2022-02-12T07:14:59.510781Z","iopub.status.idle":"2022-02-12T07:15:00.560731Z","shell.execute_reply.started":"2022-02-12T07:14:59.510734Z","shell.execute_reply":"2022-02-12T07:15:00.560049Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"def create_model(bert_model):\n    input_ids = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\n    input_mask = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\n  \n    output = bert_model([input_ids, input_mask])[1]\n    output = tf.keras.layers.Dense(32,activation='relu')(output)\n    output = tf.keras.layers.Dropout(0.2)(output)\n\n    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n    model = tf.keras.models.Model(inputs = [input_ids, input_mask],outputs = output)\n    return model\n\nbert_model = create_model(bert)\nhistory = train_model(\n    {\n        'input_ids': bert_text['input_ids'], \n        'attention_mask': bert_text['attention_mask']\n    }, \n    bert_target, \n    bert_model, \n    batch_size = 10, \n    epochs = 100\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:15:00.562068Z","iopub.execute_input":"2022-02-12T07:15:00.562345Z","iopub.status.idle":"2022-02-12T07:58:12.473459Z","shell.execute_reply.started":"2022-02-12T07:15:00.562308Z","shell.execute_reply":"2022-02-12T07:58:12.472493Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"markdown","source":"Best model prediction","metadata":{}},{"cell_type":"code","source":"def Save_Submit_nn(model, df_test):\n    predict = model.predict(df_test)\n    predict = (predict >= 0.5).astype(int)\n    print(f\"Predict shape: {predict.shape}\")\n    print(f\"Predict shape: {predict.dtype}\")\n    \n    output = pd.DataFrame({\n        'id': submission.index,\n        'target': predict.flatten()\n    })\n    output.to_csv(f'{model.name}_submission.csv', index=False)\n    print(\"Saved!\")","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:58:12.478263Z","iopub.execute_input":"2022-02-12T07:58:12.478534Z","iopub.status.idle":"2022-02-12T07:58:12.489587Z","shell.execute_reply.started":"2022-02-12T07:58:12.478498Z","shell.execute_reply":"2022-02-12T07:58:12.488597Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"#Save_Submit_nn(glove_model, X_test_pad)\ntest = {\n    'input_ids': bert_text_test['input_ids'],\n    'attention_mask': bert_text_test['attention_mask']\n}\nSave_Submit_nn(bert_model, test)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T07:58:12.494677Z","iopub.execute_input":"2022-02-12T07:58:12.497392Z","iopub.status.idle":"2022-02-12T07:58:34.646481Z","shell.execute_reply.started":"2022-02-12T07:58:12.497350Z","shell.execute_reply":"2022-02-12T07:58:34.645687Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}